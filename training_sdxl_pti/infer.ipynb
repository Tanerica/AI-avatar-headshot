{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/miniconda3/envs/tan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "Fetching 2 files: 100%|██████████| 2/2 [02:11<00:00, 65.99s/it] \n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataset_and_utils import (\n",
    "    TokenEmbeddingsHandler,\n",
    "    load_models\n",
    ")\n",
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "pretrained_model_name_or_path = \"SG161222/RealVisXL_V5.0\"\n",
    "local_weights_cache = '/workspace/AI-avatar-headshot/output_cog/Itay/'\n",
    "revision = None\n",
    "weight_dtype = torch.float32\n",
    "is_sdxl = True\n",
    "device = \"cuda\"\n",
    "(\n",
    "    tokenizer_one,\n",
    "    tokenizer_two,\n",
    "    noise_scheduler,\n",
    "    text_encoder_one,\n",
    "    text_encoder_two,\n",
    "    vae,\n",
    "    unet,\n",
    ") = load_models(pretrained_model_name_or_path, revision, device, weight_dtype, is_sdxl)\n",
    "handler = TokenEmbeddingsHandler(\n",
    "    [text_encoder_one, text_encoder_two], [tokenizer_one, tokenizer_two]\n",
    ")\n",
    "# handler.load_embeddings(os.path.join(local_weights_cache, \"embeddings.pti\"))\n",
    "handler.load_embeddings(\"/workspace/AI-avatar-headshot/output_cog/Itay/embeddings.pti\")\n",
    "pipe = StableDiffusionXLPipeline(vae = vae , text_encoder = text_encoder_one , text_encoder_2 = text_encoder_two , unet = unet , tokenizer = tokenizer_one ,  tokenizer_2 = tokenizer_two , scheduler = noise_scheduler ).to('cuda')\n",
    "pipe.load_lora_weights('/workspace/AI-avatar-headshot/training_sdxl_pti/ckpt/Itay_cog_lora.safetensors' , adapter_name = 'lora')\n",
    "pipe.set_adapters(\"lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:57<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"a <s0><s1> man wearing knitted red and black sweater, plain whitebox background, looking at the camera, detailed\"\n",
    "# for k, v in token_map.items():\n",
    "#     prompt = prompt.replace(k, v)\n",
    "# print(f\"Prompt: {prompt}\")\n",
    "neg_prompt = \"(worst quality, low quality, normal quality), disabled body, sketches, (manicure:1.2), lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, extra fingers, fewer digits, cropped, jpeg artifacts, signature, watermark, username, blurry, momochrome, (ugly) , bad hand ,  bad leg , lost fingers, 4 fingers.\"\n",
    "num_outputs = 3\n",
    "width = 768\n",
    "height = 1024\n",
    "guidance_scale = 3.5\n",
    "num_inference_steps = 25\n",
    "seed = 1234\n",
    "lora_scale = 0.7\n",
    "generator = [\n",
    "    torch.Generator(device=\"cuda\").manual_seed(seed + i)\n",
    "    for i in range(num_outputs)\n",
    "]\n",
    "sdxl_kwargs = {}\n",
    "common_args = {\n",
    "    \"prompt\": [prompt] * num_outputs if prompt is not None else None,\n",
    "    \"negative_prompt\": [neg_prompt] * num_outputs if prompt is not None else None,\n",
    "    \"guidance_scale\": guidance_scale,\n",
    "    \"generator\": generator,\n",
    "    \"num_inference_steps\": num_inference_steps,\n",
    "    \"max_sequence_length\":256\n",
    "}            \n",
    "sdxl_kwargs[\"width\"] = width\n",
    "sdxl_kwargs[\"height\"] = height\n",
    "sdxl_kwargs[\"cross_attention_kwargs\"] = {\"scale\": 1}\n",
    "results = pipe(**common_args, **sdxl_kwargs).images\n",
    "output_path = '/workspace/AI-avatar-headshot/output_cog/Itay/infer_results/'\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "for i , img in enumerate(results):\n",
    "    img.save(f'{output_path}/{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:09<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Skin texture, Closeup portrait photo of a stunning young <s1> man dressed as a highborn noble from game of thrones,f /2.8, Canon, 85mm,cinematic, high quality, looking at the camera, <s1> man\"\n",
    "\n",
    "neg_prompt = \"(worst quality, low quality, normal quality), disabled body, sketches, (manicure:1.2), lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, extra fingers, fewer digits, cropped, jpeg artifacts, signature, watermark, username, blurry, momochrome, (ugly) , bad hand ,  bad leg , lost fingers, 4 fingers.\"\n",
    "num_outputs = 3\n",
    "width = 768\n",
    "height = 1024\n",
    "guidance_scale = 6\n",
    "num_inference_steps = 30\n",
    "seed = 1234\n",
    "lora_scale = 1\n",
    "generator = [\n",
    "    torch.Generator(device=\"cuda\").manual_seed(seed + i)\n",
    "    for i in range(num_outputs)\n",
    "]\n",
    "sdxl_kwargs = {}\n",
    "common_args = {\n",
    "    \"prompt\": [prompt] * num_outputs if prompt is not None else None,\n",
    "    \"negative_prompt\": [neg_prompt] * num_outputs if prompt is not None else None,\n",
    "    \"guidance_scale\": guidance_scale,\n",
    "    \"generator\": generator,\n",
    "    \"num_inference_steps\": num_inference_steps,\n",
    "    \"max_sequence_length\":256\n",
    "}            \n",
    "sdxl_kwargs[\"width\"] = width\n",
    "sdxl_kwargs[\"height\"] = height\n",
    "sdxl_kwargs[\"cross_attention_kwargs\"] = {\"scale\": 1}\n",
    "results = pipe(**common_args, **sdxl_kwargs).images\n",
    "output_path = '/workspace/AI-avatar-headshot/output_cog/Itay/infer_results/'\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "for i , img in enumerate(results):\n",
    "    img.save(f'{output_path}/{i+6}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:45<00:00,  2.30s/it]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "prompt = \"beautiful young <s1> man, peach orange teal sweather, bright face, depth of field urban by Annie Leibovitz\"\n",
    "\n",
    "neg_prompt = \"(worst quality, low quality, normal quality), disabled body, sketches, (manicure:1.2), lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, extra fingers, fewer digits, cropped, jpeg artifacts, signature, watermark, username, blurry, momochrome, (ugly) , bad hand ,  bad leg , lost fingers, 4 fingers.\"\n",
    "num_outputs = 3\n",
    "width = 768\n",
    "height = 1024\n",
    "guidance_scale = 3.5\n",
    "num_inference_steps = 20\n",
    "seed = 1234\n",
    "lora_scale = 0.7\n",
    "generator = [\n",
    "    torch.Generator(device=\"cuda\").manual_seed(seed + i)\n",
    "    for i in range(num_outputs)\n",
    "]\n",
    "sdxl_kwargs = {}\n",
    "common_args = {\n",
    "    \"prompt\": [prompt] * num_outputs if prompt is not None else None,\n",
    "    \"negative_prompt\": [neg_prompt] * num_outputs if prompt is not None else None,\n",
    "    \"guidance_scale\": guidance_scale,\n",
    "    \"generator\": generator,\n",
    "    \"num_inference_steps\": num_inference_steps,\n",
    "    \"max_sequence_length\":256\n",
    "}            \n",
    "sdxl_kwargs[\"width\"] = width\n",
    "sdxl_kwargs[\"height\"] = height\n",
    "sdxl_kwargs[\"cross_attention_kwargs\"] = {\"scale\": 1}\n",
    "results = pipe(**common_args, **sdxl_kwargs).images\n",
    "output_path = '/workspace/AI-avatar-headshot/output_cog/Itay/infer_results/'\n",
    "for i , img in enumerate(results):\n",
    "    img.save(f'{output_path}/{i+9}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (83 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', pretty, amazing, symmetry', ', pretty, amazing, symmetry', ', pretty, amazing, symmetry']\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (83 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', pretty, amazing, symmetry', ', pretty, amazing, symmetry', ', pretty, amazing, symmetry']\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:54<00:00,  1.82s/it]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "prompt = \"a portrait of a ali01 man, close up photo of smiling, headshot for linkedin, professional, detailed, sharp focus, warm light, attractive, full background, directed, vivid colors, perfect composition, elegant, intricate, beautiful, highly saturated color, epic, stunning, gorgeous, cinematic, striking, rich deep detail, romantic, inspired, vibrant, illuminated, fancy, pretty, amazing, symmetry\"\n",
    "\n",
    "neg_prompt = \"(worst quality, low quality, normal quality), disabled body, sketches, (manicure:1.2), lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, extra fingers, fewer digits, cropped, jpeg artifacts, signature, watermark, username, blurry, momochrome, (ugly) , bad hand ,  bad leg , lost fingers, 4 fingers.\"\n",
    "num_outputs = 3\n",
    "width = 768\n",
    "height = 1024\n",
    "guidance_scale = 3.5\n",
    "num_inference_steps = 30\n",
    "seed = 1234\n",
    "lora_scale = 0.9\n",
    "generator = [\n",
    "    torch.Generator(device=\"cuda\").manual_seed(seed + i)\n",
    "    for i in range(num_outputs)\n",
    "]\n",
    "sdxl_kwargs = {}\n",
    "common_args = {\n",
    "    \"prompt\": [prompt] * num_outputs if prompt is not None else None,\n",
    "    \"negative_prompt\": [neg_prompt] * num_outputs if prompt is not None else None,\n",
    "    \"guidance_scale\": guidance_scale,\n",
    "    \"generator\": generator,\n",
    "    \"num_inference_steps\": num_inference_steps,\n",
    "    \"max_sequence_length\":256\n",
    "}            \n",
    "sdxl_kwargs[\"width\"] = width\n",
    "sdxl_kwargs[\"height\"] = height\n",
    "sdxl_kwargs[\"cross_attention_kwargs\"] = {\"scale\": 1}\n",
    "results = pipe(**common_args, **sdxl_kwargs).images\n",
    "for i , img in enumerate(results):\n",
    "    img.save(f'{output_path}/{i+15}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping encoder.mid.attn_1.q.weight for SD format\n",
      "Reshaping encoder.mid.attn_1.k.weight for SD format\n",
      "Reshaping encoder.mid.attn_1.v.weight for SD format\n",
      "Reshaping encoder.mid.attn_1.proj_out.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.q.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.k.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.v.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.proj_out.weight for SD format\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Script for converting a HF Diffusers saved pipeline to a Stable Diffusion checkpoint.\n",
    "# *Only* converts the UNet, VAE, and Text Encoder.\n",
    "# Does not convert optimizer state or any other thing.\n",
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from safetensors.torch import load_file, save_file\n",
    "\n",
    "\n",
    "# =================#\n",
    "# UNet Conversion #\n",
    "# =================#\n",
    "\n",
    "unet_conversion_map = [\n",
    "    # (stable-diffusion, HF Diffusers)\n",
    "    (\"time_embed.0.weight\", \"time_embedding.linear_1.weight\"),\n",
    "    (\"time_embed.0.bias\", \"time_embedding.linear_1.bias\"),\n",
    "    (\"time_embed.2.weight\", \"time_embedding.linear_2.weight\"),\n",
    "    (\"time_embed.2.bias\", \"time_embedding.linear_2.bias\"),\n",
    "    (\"input_blocks.0.0.weight\", \"conv_in.weight\"),\n",
    "    (\"input_blocks.0.0.bias\", \"conv_in.bias\"),\n",
    "    (\"out.0.weight\", \"conv_norm_out.weight\"),\n",
    "    (\"out.0.bias\", \"conv_norm_out.bias\"),\n",
    "    (\"out.2.weight\", \"conv_out.weight\"),\n",
    "    (\"out.2.bias\", \"conv_out.bias\"),\n",
    "    # the following are for sdxl\n",
    "    (\"label_emb.0.0.weight\", \"add_embedding.linear_1.weight\"),\n",
    "    (\"label_emb.0.0.bias\", \"add_embedding.linear_1.bias\"),\n",
    "    (\"label_emb.0.2.weight\", \"add_embedding.linear_2.weight\"),\n",
    "    (\"label_emb.0.2.bias\", \"add_embedding.linear_2.bias\"),\n",
    "]\n",
    "\n",
    "unet_conversion_map_resnet = [\n",
    "    # (stable-diffusion, HF Diffusers)\n",
    "    (\"in_layers.0\", \"norm1\"),\n",
    "    (\"in_layers.2\", \"conv1\"),\n",
    "    (\"out_layers.0\", \"norm2\"),\n",
    "    (\"out_layers.3\", \"conv2\"),\n",
    "    (\"emb_layers.1\", \"time_emb_proj\"),\n",
    "    (\"skip_connection\", \"conv_shortcut\"),\n",
    "]\n",
    "\n",
    "unet_conversion_map_layer = []\n",
    "# hardcoded number of downblocks and resnets/attentions...\n",
    "# would need smarter logic for other networks.\n",
    "for i in range(3):\n",
    "    # loop over downblocks/upblocks\n",
    "\n",
    "    for j in range(2):\n",
    "        # loop over resnets/attentions for downblocks\n",
    "        hf_down_res_prefix = f\"down_blocks.{i}.resnets.{j}.\"\n",
    "        sd_down_res_prefix = f\"input_blocks.{3*i + j + 1}.0.\"\n",
    "        unet_conversion_map_layer.append((sd_down_res_prefix, hf_down_res_prefix))\n",
    "\n",
    "        if i > 0:\n",
    "            hf_down_atn_prefix = f\"down_blocks.{i}.attentions.{j}.\"\n",
    "            sd_down_atn_prefix = f\"input_blocks.{3*i + j + 1}.1.\"\n",
    "            unet_conversion_map_layer.append((sd_down_atn_prefix, hf_down_atn_prefix))\n",
    "\n",
    "    for j in range(4):\n",
    "        # loop over resnets/attentions for upblocks\n",
    "        hf_up_res_prefix = f\"up_blocks.{i}.resnets.{j}.\"\n",
    "        sd_up_res_prefix = f\"output_blocks.{3*i + j}.0.\"\n",
    "        unet_conversion_map_layer.append((sd_up_res_prefix, hf_up_res_prefix))\n",
    "\n",
    "        if i < 2:\n",
    "            # no attention layers in up_blocks.0\n",
    "            hf_up_atn_prefix = f\"up_blocks.{i}.attentions.{j}.\"\n",
    "            sd_up_atn_prefix = f\"output_blocks.{3 * i + j}.1.\"\n",
    "            unet_conversion_map_layer.append((sd_up_atn_prefix, hf_up_atn_prefix))\n",
    "\n",
    "    if i < 3:\n",
    "        # no downsample in down_blocks.3\n",
    "        hf_downsample_prefix = f\"down_blocks.{i}.downsamplers.0.conv.\"\n",
    "        sd_downsample_prefix = f\"input_blocks.{3*(i+1)}.0.op.\"\n",
    "        unet_conversion_map_layer.append((sd_downsample_prefix, hf_downsample_prefix))\n",
    "\n",
    "        # no upsample in up_blocks.3\n",
    "        hf_upsample_prefix = f\"up_blocks.{i}.upsamplers.0.\"\n",
    "        sd_upsample_prefix = f\"output_blocks.{3*i + 2}.{1 if i == 0 else 2}.\"\n",
    "        unet_conversion_map_layer.append((sd_upsample_prefix, hf_upsample_prefix))\n",
    "unet_conversion_map_layer.append((\"output_blocks.2.2.conv.\", \"output_blocks.2.1.conv.\"))\n",
    "\n",
    "hf_mid_atn_prefix = \"mid_block.attentions.0.\"\n",
    "sd_mid_atn_prefix = \"middle_block.1.\"\n",
    "unet_conversion_map_layer.append((sd_mid_atn_prefix, hf_mid_atn_prefix))\n",
    "for j in range(2):\n",
    "    hf_mid_res_prefix = f\"mid_block.resnets.{j}.\"\n",
    "    sd_mid_res_prefix = f\"middle_block.{2*j}.\"\n",
    "    unet_conversion_map_layer.append((sd_mid_res_prefix, hf_mid_res_prefix))\n",
    "\n",
    "\n",
    "def convert_unet_state_dict(unet_state_dict):\n",
    "    # buyer beware: this is a *brittle* function,\n",
    "    # and correct output requires that all of these pieces interact in\n",
    "    # the exact order in which I have arranged them.\n",
    "    mapping = {k: k for k in unet_state_dict.keys()}\n",
    "    for sd_name, hf_name in unet_conversion_map:\n",
    "        mapping[hf_name] = sd_name\n",
    "    for k, v in mapping.items():\n",
    "        if \"resnets\" in k:\n",
    "            for sd_part, hf_part in unet_conversion_map_resnet:\n",
    "                v = v.replace(hf_part, sd_part)\n",
    "            mapping[k] = v\n",
    "    for k, v in mapping.items():\n",
    "        for sd_part, hf_part in unet_conversion_map_layer:\n",
    "            v = v.replace(hf_part, sd_part)\n",
    "        mapping[k] = v\n",
    "    new_state_dict = {sd_name: unet_state_dict[hf_name] for hf_name, sd_name in mapping.items()}\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "# ================#\n",
    "# VAE Conversion #\n",
    "# ================#\n",
    "\n",
    "vae_conversion_map = [\n",
    "    # (stable-diffusion, HF Diffusers)\n",
    "    (\"nin_shortcut\", \"conv_shortcut\"),\n",
    "    (\"norm_out\", \"conv_norm_out\"),\n",
    "    (\"mid.attn_1.\", \"mid_block.attentions.0.\"),\n",
    "]\n",
    "\n",
    "for i in range(4):\n",
    "    # down_blocks have two resnets\n",
    "    for j in range(2):\n",
    "        hf_down_prefix = f\"encoder.down_blocks.{i}.resnets.{j}.\"\n",
    "        sd_down_prefix = f\"encoder.down.{i}.block.{j}.\"\n",
    "        vae_conversion_map.append((sd_down_prefix, hf_down_prefix))\n",
    "\n",
    "    if i < 3:\n",
    "        hf_downsample_prefix = f\"down_blocks.{i}.downsamplers.0.\"\n",
    "        sd_downsample_prefix = f\"down.{i}.downsample.\"\n",
    "        vae_conversion_map.append((sd_downsample_prefix, hf_downsample_prefix))\n",
    "\n",
    "        hf_upsample_prefix = f\"up_blocks.{i}.upsamplers.0.\"\n",
    "        sd_upsample_prefix = f\"up.{3-i}.upsample.\"\n",
    "        vae_conversion_map.append((sd_upsample_prefix, hf_upsample_prefix))\n",
    "\n",
    "    # up_blocks have three resnets\n",
    "    # also, up blocks in hf are numbered in reverse from sd\n",
    "    for j in range(3):\n",
    "        hf_up_prefix = f\"decoder.up_blocks.{i}.resnets.{j}.\"\n",
    "        sd_up_prefix = f\"decoder.up.{3-i}.block.{j}.\"\n",
    "        vae_conversion_map.append((sd_up_prefix, hf_up_prefix))\n",
    "\n",
    "# this part accounts for mid blocks in both the encoder and the decoder\n",
    "for i in range(2):\n",
    "    hf_mid_res_prefix = f\"mid_block.resnets.{i}.\"\n",
    "    sd_mid_res_prefix = f\"mid.block_{i+1}.\"\n",
    "    vae_conversion_map.append((sd_mid_res_prefix, hf_mid_res_prefix))\n",
    "\n",
    "\n",
    "vae_conversion_map_attn = [\n",
    "    # (stable-diffusion, HF Diffusers)\n",
    "    (\"norm.\", \"group_norm.\"),\n",
    "    # the following are for SDXL\n",
    "    (\"q.\", \"to_q.\"),\n",
    "    (\"k.\", \"to_k.\"),\n",
    "    (\"v.\", \"to_v.\"),\n",
    "    (\"proj_out.\", \"to_out.0.\"),\n",
    "]\n",
    "\n",
    "\n",
    "def reshape_weight_for_sd(w):\n",
    "    # convert HF linear weights to SD conv2d weights\n",
    "    if not w.ndim == 1:\n",
    "        return w.reshape(*w.shape, 1, 1)\n",
    "    else:\n",
    "        return w\n",
    "\n",
    "\n",
    "def convert_vae_state_dict(vae_state_dict):\n",
    "    mapping = {k: k for k in vae_state_dict.keys()}\n",
    "    for k, v in mapping.items():\n",
    "        for sd_part, hf_part in vae_conversion_map:\n",
    "            v = v.replace(hf_part, sd_part)\n",
    "        mapping[k] = v\n",
    "    for k, v in mapping.items():\n",
    "        if \"attentions\" in k:\n",
    "            for sd_part, hf_part in vae_conversion_map_attn:\n",
    "                v = v.replace(hf_part, sd_part)\n",
    "            mapping[k] = v\n",
    "    new_state_dict = {v: vae_state_dict[k] for k, v in mapping.items()}\n",
    "    weights_to_convert = [\"q\", \"k\", \"v\", \"proj_out\"]\n",
    "    for k, v in new_state_dict.items():\n",
    "        for weight_name in weights_to_convert:\n",
    "            if f\"mid.attn_1.{weight_name}.weight\" in k:\n",
    "                print(f\"Reshaping {k} for SD format\")\n",
    "                new_state_dict[k] = reshape_weight_for_sd(v)\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "# =========================#\n",
    "# Text Encoder Conversion #\n",
    "# =========================#\n",
    "\n",
    "\n",
    "textenc_conversion_lst = [\n",
    "    # (stable-diffusion, HF Diffusers)\n",
    "    (\"transformer.resblocks.\", \"text_model.encoder.layers.\"),\n",
    "    (\"ln_1\", \"layer_norm1\"),\n",
    "    (\"ln_2\", \"layer_norm2\"),\n",
    "    (\".c_fc.\", \".fc1.\"),\n",
    "    (\".c_proj.\", \".fc2.\"),\n",
    "    (\".attn\", \".self_attn\"),\n",
    "    (\"ln_final.\", \"text_model.final_layer_norm.\"),\n",
    "    (\"token_embedding.weight\", \"text_model.embeddings.token_embedding.weight\"),\n",
    "    (\"positional_embedding\", \"text_model.embeddings.position_embedding.weight\"),\n",
    "]\n",
    "protected = {re.escape(x[1]): x[0] for x in textenc_conversion_lst}\n",
    "textenc_pattern = re.compile(\"|\".join(protected.keys()))\n",
    "\n",
    "# Ordering is from https://github.com/pytorch/pytorch/blob/master/test/cpp/api/modules.cpp\n",
    "code2idx = {\"q\": 0, \"k\": 1, \"v\": 2}\n",
    "\n",
    "\n",
    "def convert_openclip_text_enc_state_dict(text_enc_dict):\n",
    "    new_state_dict = {}\n",
    "    capture_qkv_weight = {}\n",
    "    capture_qkv_bias = {}\n",
    "    for k, v in text_enc_dict.items():\n",
    "        if (\n",
    "            k.endswith(\".self_attn.q_proj.weight\")\n",
    "            or k.endswith(\".self_attn.k_proj.weight\")\n",
    "            or k.endswith(\".self_attn.v_proj.weight\")\n",
    "        ):\n",
    "            k_pre = k[: -len(\".q_proj.weight\")]\n",
    "            k_code = k[-len(\"q_proj.weight\")]\n",
    "            if k_pre not in capture_qkv_weight:\n",
    "                capture_qkv_weight[k_pre] = [None, None, None]\n",
    "            capture_qkv_weight[k_pre][code2idx[k_code]] = v\n",
    "            continue\n",
    "\n",
    "        if (\n",
    "            k.endswith(\".self_attn.q_proj.bias\")\n",
    "            or k.endswith(\".self_attn.k_proj.bias\")\n",
    "            or k.endswith(\".self_attn.v_proj.bias\")\n",
    "        ):\n",
    "            k_pre = k[: -len(\".q_proj.bias\")]\n",
    "            k_code = k[-len(\"q_proj.bias\")]\n",
    "            if k_pre not in capture_qkv_bias:\n",
    "                capture_qkv_bias[k_pre] = [None, None, None]\n",
    "            capture_qkv_bias[k_pre][code2idx[k_code]] = v\n",
    "            continue\n",
    "\n",
    "        relabelled_key = textenc_pattern.sub(lambda m: protected[re.escape(m.group(0))], k)\n",
    "        new_state_dict[relabelled_key] = v\n",
    "\n",
    "    for k_pre, tensors in capture_qkv_weight.items():\n",
    "        if None in tensors:\n",
    "            raise Exception(\"CORRUPTED MODEL: one of the q-k-v values for the text encoder was missing\")\n",
    "        relabelled_key = textenc_pattern.sub(lambda m: protected[re.escape(m.group(0))], k_pre)\n",
    "        new_state_dict[relabelled_key + \".in_proj_weight\"] = torch.cat(tensors)\n",
    "\n",
    "    for k_pre, tensors in capture_qkv_bias.items():\n",
    "        if None in tensors:\n",
    "            raise Exception(\"CORRUPTED MODEL: one of the q-k-v values for the text encoder was missing\")\n",
    "        relabelled_key = textenc_pattern.sub(lambda m: protected[re.escape(m.group(0))], k_pre)\n",
    "        new_state_dict[relabelled_key + \".in_proj_bias\"] = torch.cat(tensors)\n",
    "\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "def convert_openai_text_enc_state_dict(text_enc_dict):\n",
    "    return text_enc_dict\n",
    "\n",
    "\n",
    "\n",
    "model_dict = {\n",
    "'text_encoder_one': text_encoder_one.state_dict(),\n",
    "'text_encoder_two': text_encoder_two.state_dict(),\n",
    "'vae': vae.state_dict(),\n",
    "'unet': unet.state_dict(),\n",
    "'text_encoder_one_embeddings': embeddings_one,\n",
    "'text_encoder_two_embeddings': embeddings_two,\n",
    "}\n",
    "unet_state_dict = model_dict['unet']\n",
    "vae_state_dict = model_dict['vae']\n",
    "text_enc_dict = model_dict['text_encoder_one']\n",
    "text_enc_2_dict = model_dict['text_encoder_two']\n",
    "# Convert the UNet model\n",
    "unet_state_dict = convert_unet_state_dict(unet_state_dict)\n",
    "unet_state_dict = {\"model.diffusion_model.\" + k: v for k, v in unet_state_dict.items()}\n",
    "\n",
    "# Convert the VAE model\n",
    "vae_state_dict = convert_vae_state_dict(vae_state_dict)\n",
    "vae_state_dict = {\"first_stage_model.\" + k: v for k, v in vae_state_dict.items()}\n",
    "\n",
    "# Convert text encoder 1\n",
    "text_enc_dict = convert_openai_text_enc_state_dict(text_enc_dict)\n",
    "text_enc_dict = {\"conditioner.embedders.0.transformer.\" + k: v for k, v in text_enc_dict.items()}\n",
    "\n",
    "# Convert text encoder 2\n",
    "text_enc_2_dict = convert_openclip_text_enc_state_dict(text_enc_2_dict)\n",
    "text_enc_2_dict = {\"conditioner.embedders.1.model.\" + k: v for k, v in text_enc_2_dict.items()}\n",
    "# We call the `.T.contiguous()` to match what's done in\n",
    "# https://github.com/huggingface/diffusers/blob/84905ca7287876b925b6bf8e9bb92fec21c78764/src/diffusers/loaders/single_file_utils.py#L1085\n",
    "text_enc_2_dict[\"conditioner.embedders.1.model.text_projection\"] = text_enc_2_dict.pop(\n",
    "    \"conditioner.embedders.1.model.text_projection.weight\"\n",
    ").T.contiguous()\n",
    "\n",
    "# Put together new checkpoint\n",
    "state_dict = {**unet_state_dict, **vae_state_dict, **text_enc_dict, **text_enc_2_dict}\n",
    "\n",
    "\n",
    "state_dict = {k: v.half() for k, v in state_dict.items()}\n",
    "\n",
    "\n",
    "save_file(state_dict, '/workspace/output_cog/Ali/updated_model.safetensors')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
